{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set default plot style\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10,8\n",
    "plt.rcParams['figure.frameon'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "# get data\n",
    "pm_df = pd.read_json('./data/pmsensors.json')\n",
    "tf_df = pd.read_json('./data/trafficflows.json')\n",
    "ti_df = pd.read_json('./data/trafficincidents.json')\n",
    "\n",
    "print('pm_df length:', len(pm_df))\n",
    "print('tf_df length:', len(tf_df))\n",
    "print('ti_df length:', len(ti_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls\n",
    "pm_df = pm_df.dropna()\n",
    "ti_df = ti_df.dropna()\n",
    "tf_df = tf_df.dropna()\n",
    "\n",
    "# drop MongoDB id and version\n",
    "pm_df = pm_df.drop(['_id', '__v'], axis=1)\n",
    "tf_df = tf_df.drop(['_id', '__v'], axis=1)\n",
    "ti_df = ti_df.drop(['_id', '__v'], axis=1)\n",
    "\n",
    "print('pm_df length:', len(pm_df))\n",
    "print('tf_df length:', len(tf_df))\n",
    "print('ti_df length:', len(ti_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timestamps of all datapoints\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.eventplot(pm_df['timestamp'], orientation='horizontal', lineoffsets=-0.25, linelengths=0.2, colors='r')\n",
    "plt.eventplot(tf_df['timestamp'], orientation='horizontal', lineoffsets=0, linelengths=0.2, colors='g')\n",
    "plt.eventplot(ti_df['timestamp'], orientation='horizontal', lineoffsets=0.25, linelengths=0.2, colors='b')\n",
    "\n",
    "plt.legend(['pm sensor readings','traffic flow readings','traffic incident readings'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data from early sessions (connection unreliable)\n",
    "pm_df = pm_df[pm_df['timestamp'] >= '2021-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add session numbers to particulate data\n",
    "pm_df['delta'] = pm_df['timestamp'].diff()\n",
    "pm_df = pm_df.dropna() # drop first row after calling diff \n",
    "\n",
    "# remove deltas < 1 s (caused by unreliable data surges after a connection drop out)\n",
    "pm_df = pm_df.drop(pm_df[pm_df['delta'] < pd.Timedelta(0.5, unit='s')].index)\n",
    "\n",
    "# find large deltas > 100 s (implies start of new journey)\n",
    "max_dropout = 100 # max acceptable dropout before starting new session\n",
    "pm_df['large_delta'] = pm_df['delta'].apply(lambda x: x > pd.Timedelta(max_dropout, unit='s'))\n",
    "pm_df['large_delta'] = pm_df['large_delta'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# assign session\n",
    "pm_df['sessionNo'] = pm_df['large_delta'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_interval(df, session_no):\n",
    "    # finds the start and end times for a given session\n",
    "    session_data = df[df['sessionNo'] == session_no]\n",
    "    start = session_data['timestamp'].min()\n",
    "    end = session_data['timestamp'].max()\n",
    "    return {\n",
    "        'start': start,\n",
    "        'end': end\n",
    "    }\n",
    "\n",
    "last_session = pm_df['sessionNo'].max()\n",
    "\n",
    "# drop sessions shorter than 3 mins\n",
    "for i in range(last_session + 1):\n",
    "    session_interval = get_session_interval(pm_df, i)\n",
    "    duration = session_interval['end'] - session_interval['start']\n",
    "\n",
    "    if duration < pd.Timedelta(180, unit='s'):\n",
    "        pm_df = pm_df.drop(pm_df[pm_df['sessionNo'] == i].index)\n",
    "\n",
    "# reassign session numbers (maintains consistent increments)\n",
    "pm_df = pm_df.drop(['sessionNo'], axis=1)\n",
    "pm_df['sessionNo'] = pm_df['large_delta'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df_std = pm_df[['pm1_0', 'pm2_5', 'pm10_0', 'delta', 'large_delta']]\n",
    "pm_df_std = pm_df_std[pm_df_std['large_delta'] != 1]\n",
    "pm_df_std = pm_df_std.drop(['large_delta'], axis=1)\n",
    "pm_df_std['delta'] = pm_df_std['delta'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "z_scores = stats.zscore(pm_df_std)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "\n",
    "pm_df_std = pm_df_std[filtered_entries]\n",
    "pm_df_std.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df_std.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df_std['delta'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = pm_df.drop(['delta', 'large_delta'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign sessions to tf_df and ti_df (and remove any entries outside ranges)\n",
    "session_intervals = []\n",
    "\n",
    "last_session = pm_df['sessionNo'].max()\n",
    "\n",
    "for i in range(last_session + 1):\n",
    "    session_intervals.append(get_session_interval(pm_df, i))\n",
    "\n",
    "def assign_session(x, intervals):\n",
    "    # assign session numbers to ti_df and tf_df frames\n",
    "    # set any timestamps outside session intervals to NaN (to then drop from df later)\n",
    "    # reasons for timestamps outside session intervals include leaving OwnTracks open...\n",
    "    # ... after completing a journey\n",
    "\n",
    "    for (i, interval) in enumerate(intervals):\n",
    "        if x >= interval['start'] and x <= interval['end']:\n",
    "            # point is within a valid interval\n",
    "            return i\n",
    "    # point outside valid intervals\n",
    "    return np.nan\n",
    "\n",
    "tf_df['sessionNo'] = tf_df['timestamp'].apply(assign_session, args=([session_intervals]))\n",
    "ti_df['sessionNo'] = ti_df['timestamp'].apply(assign_session, args=([session_intervals]))\n",
    "\n",
    "tf_df = tf_df.dropna()\n",
    "ti_df = ti_df.dropna()\n",
    "\n",
    "# cast to ints\n",
    "tf_df['sessionNo'] = tf_df['sessionNo'].astype(int)\n",
    "ti_df['sessionNo'] = ti_df['sessionNo'].astype(int)\n",
    "\n",
    "print('pm_df length:', len(pm_df))\n",
    "print('tf_df length:', len(tf_df))\n",
    "print('ti_df length:', len(ti_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat to drop any pm_df values that fall outside collected traffic data\n",
    "session_intervals = []\n",
    "\n",
    "last_session = tf_df['sessionNo'].max()\n",
    "\n",
    "for i in range(last_session + 1):\n",
    "    session_intervals.append(get_session_interval(tf_df, i))\n",
    "\n",
    "pm_df['inTFSession'] = pm_df['timestamp'].apply(assign_session, args=([session_intervals]))\n",
    "\n",
    "pm_df = pm_df.dropna()\n",
    "pm_df = pm_df.drop(['inTFSession'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassess intervals\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.eventplot(pm_df['timestamp'], orientation='horizontal', lineoffsets=-0.25, linelengths=0.2, colors='r')\n",
    "plt.eventplot(tf_df['timestamp'], orientation='horizontal', lineoffsets=0, linelengths=0.2, colors='g')\n",
    "plt.eventplot(ti_df['timestamp'], orientation='horizontal', lineoffsets=0.25, linelengths=0.2, colors='b')\n",
    "\n",
    "plt.legend(['pmsensors','trafficflows','trafficincidents'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average of particulate matter values \n",
    "pm_readings = pm_df[['pm1_0', 'pm2_5', 'pm10_0']]\n",
    "pm_avg = pm_readings.sum(axis=1)/3\n",
    "pm_df['pmAvg'] = pm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a delay factor to the traffic flow frame (equivalent to factor of reduction in speed vs free flow)\n",
    "tf_df['delayFactor'] = tf_df.apply(lambda x: x['freeFlowSpeed']/x['currentSpeed'], axis=1)\n",
    "print('max delay factor', tf_df['delayFactor'].max())\n",
    "print('min delay factor', tf_df['delayFactor'].min())\n",
    "\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure values are chronological\n",
    "pm_df.sort_values(by=['timestamp'], inplace=True)\n",
    "tf_df.sort_values(by=['timestamp'], inplace=True)\n",
    "ti_df.sort_values(by=['timestamp'], inplace=True)\n",
    "\n",
    "# index rows by time\n",
    "time_index = pd.DatetimeIndex(pm_df['timestamp'])\n",
    "pm_df = pm_df.set_index(time_index)\n",
    "pm_df = pm_df.drop(['timestamp'], axis=1)\n",
    "\n",
    "time_index = pd.DatetimeIndex(tf_df['timestamp'])\n",
    "tf_df = tf_df.set_index(time_index)\n",
    "tf_df = tf_df.drop(['timestamp'], axis=1)\n",
    "\n",
    "time_index = pd.DatetimeIndex(ti_df['timestamp'])\n",
    "ti_df = ti_df.set_index(time_index)\n",
    "ti_df = ti_df.drop(['timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a session for analysis\n",
    "session_no = 10\n",
    "\n",
    "pm_df_s = pm_df[pm_df['sessionNo'] == session_no]\n",
    "tf_df_s = tf_df[tf_df['sessionNo'] == session_no]\n",
    "ti_df_s = ti_df[ti_df['sessionNo'] == session_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting traffic delay factor versus location as a scatter plot\n",
    "plot_session = tf_df\n",
    "\n",
    "max_delay_factor = tf_df['delayFactor'].max()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(\n",
    "    plot_session['lon'],\n",
    "    plot_session['lat'],\n",
    "    c=plot_session['delayFactor'],\n",
    "    cmap=plt.get_cmap('jet'),\n",
    "    norm=colors.LogNorm(\n",
    "        vmin=1,\n",
    "        vmax=max_delay_factor*0.6,\n",
    "        clip=True),\n",
    "    alpha=0.3\n",
    "    )\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth particulate session data and remove anomalies\n",
    "def mean_absolute_percentage_error(y_true, y_pred): # delete if not used\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# moving_average and find_anomalies modified from smarthome_data_processing\n",
    "def moving_average(series, filter_window=6, ax=None):\n",
    "    # returns the filtered series, and an axes to plot if required\n",
    "    rolling_mean = series.rolling(window=filter_window).mean()\n",
    "    if ax:\n",
    "        ax.plot(rolling_mean, label='rolling mean ({})'.format(series.name))\n",
    "        ax.plot(series[filter_window:], label='original data ({})'.format(series.name))\n",
    "        ax.legend(loc='upper left')\n",
    "    return rolling_mean\n",
    "\n",
    "\n",
    "def find_anomalies(series, filter_window=6, ax=None, scale=2.576): # 2.576 = 99% confidence interval \n",
    "    rolling_mean = moving_average(series, filter_window, ax)\n",
    "    mae = mean_absolute_error(series[filter_window:], rolling_mean[filter_window:])\n",
    "\n",
    "    deviation = np.std(series[filter_window:] - rolling_mean[filter_window:])\n",
    "    lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "    upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "\n",
    "    anomalies = pd.Series(index=series.index, name=series.name)\n",
    "    anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "    anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "    \n",
    "    if ax:\n",
    "        ax.plot(upper_bond, \"r--\", label=\"upper bound/lower bound\")\n",
    "        ax.plot(lower_bond, \"r--\")\n",
    "        ax.plot(anomalies, \"ro\", markersize=10)\n",
    "\n",
    "    return anomalies # time-indexed series, anomalous results == anomalous value, otherwise null\n",
    "\n",
    "filter_cols = ['pm1_0', 'pm2_5', 'pm10_0', 'pmAvg']\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(20,20))\n",
    "\n",
    "for i, col in enumerate(filter_cols):\n",
    "    moving_average(pm_df_s[col], ax=axs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(20,20))\n",
    "\n",
    "for i, col in enumerate(filter_cols):\n",
    "    find_anomalies(pm_df_s[col], ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_pm_anomalies():\n",
    "    # label anomalies in entire pm dataset\n",
    "    pm_df_c = pm_df.copy()\n",
    "    pm_df_c.drop(pm_df_c.index, inplace=True) # empty dataframe\n",
    "\n",
    "    for session in range(last_session + 1):\n",
    "        # for each session\n",
    "        session_data = pm_df[pm_df['sessionNo'] == session]\n",
    "\n",
    "        for col in filter_cols:\n",
    "            # for each col ['pm1_0', 'pm2_5', 'pm10_0', 'pmAvg']\n",
    "            col_name = col + 'Anom'\n",
    "            session_data[col_name] = find_anomalies(session_data[col])\n",
    "        \n",
    "        pm_df_c = pd.concat([pm_df_c, session_data])\n",
    "\n",
    "    return pm_df_c\n",
    "\n",
    "pm_df = find_all_pm_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set anomalies to nan\n",
    "for col in filter_cols:\n",
    "    anom_col = col + 'Anom'\n",
    "    pm_df[anom_col] = pm_df[anom_col].notna()\n",
    "\n",
    "\n",
    "def set_to_null(value, anom_flag):\n",
    "    return np.nan if anom_flag else value\n",
    "\n",
    "for col in filter_cols:\n",
    "    anom_col = col + 'Anom'\n",
    "    pm_df[col] = pm_df.apply(lambda row: set_to_null(row[col], row[anom_col]), axis = 1)\n",
    "\n",
    "pm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# farewell helper columns, we thank you\n",
    "pm_df = pm_df.drop(['pm1_0Anom', 'pm2_5Anom', 'pm10_0Anom', 'pmAvgAnom'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate anomalous values\n",
    "pm_df = pm_df.interpolate(method='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm result with session\n",
    "pm_df_s = pm_df[pm_df['sessionNo'] == session_no]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(20,8))\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "ax = plt.axes()\n",
    "\n",
    "moving_average(pm_df_s['pm1_0'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_pm_data():\n",
    "    # smooth all data sessions\n",
    "    pm_df_c = pm_df.copy()\n",
    "    pm_df_c.drop(pm_df_c.index, inplace=True) # empty dataframe\n",
    "\n",
    "    for session in range(last_session + 1):\n",
    "        # for each session\n",
    "        session_data = pm_df[pm_df['sessionNo'] == session]\n",
    "\n",
    "        for col in filter_cols:\n",
    "            # for each col ['pm1_0', 'pm2_5', 'pm10_0', 'pmAvg']\n",
    "            col_name = col + 'Smoothed'\n",
    "            session_data[col_name] = moving_average(session_data[col])\n",
    "        \n",
    "        pm_df_c = pd.concat([pm_df_c, session_data])\n",
    "\n",
    "    # drop nulls (first 6 entries of each window)\n",
    "    pm_df_c = pm_df_c.dropna()\n",
    "    return pm_df_c\n",
    "\n",
    "pm_df = smooth_pm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm result with session\n",
    "pm_df_s = pm_df[pm_df['sessionNo'] == session_no]\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(pm_df_s['pm1_0Smoothed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to assess correlations, need to first combine the datasets\n",
    "tf_df = tf_df.reindex(pm_df.index, method='ffill')\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pm_df.join(tf_df, rsuffix='TF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['sessionNoTF'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    'pm1_0Smoothed',\n",
    "    'pm2_5Smoothed',\n",
    "    'pm10_0Smoothed',\n",
    "    'pmAvgSmoothed',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'alt',\n",
    "    'currentSpeed',\n",
    "    'freeFlowSpeed',\n",
    "    'delayFactor',\n",
    "    ]\n",
    "\n",
    "pd.plotting.scatter_matrix(df[attributes], figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MongoDB\n",
    "client =  MongoClient('<<DATABASE URL>>')\n",
    "db = client['SIOTData']\n",
    "collection = db['processeddatas']\n",
    "df.reset_index(inplace=True)\n",
    "data_dict = df.to_dict(\"records\")\n",
    "\n",
    "\n",
    "# insert collection\n",
    "collection.insert_many(data_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc0340e282a639527904eb2aae44e44a984bba2b96cedf72d196490fd24cf687"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
